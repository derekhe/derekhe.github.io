<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="aZ679hf2Rw"><title> 我是思聪</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/prontera.css"><link rel="search" type="application/opensearchdescription+xml" href="http://www.april1985.com/atom.xml" title="我是思聪"><!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-129409069-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-129409069-1');
</script>


<script>
  (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      }
      else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
  })();
  </script></head><body><header class="feature-header"><nav class="component-nav"><ul><div class="logo-container"><a href="/"><h2 class="title">我是思聪</h2></a></div><a href="/" target="_self" class="li component-nav-item"><p>现在</p></a><a href="/archives" target="_self" class="li component-nav-item"><p>曾经</p></a><ul class="shortcut-icons"><a href="https://github.com/derekhe" target="_blank"><img src="/images/github.svg" class="icon"></a><a href="/atom.xml" target="_blank"><img src="/images/rss.svg" class="icon"></a></ul></ul></nav></header><main class="container"><div id="body-container"><div id="post-list" class="left"><div class="home post-list"><div class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/post/2019-05-27-proxy/" class="post-title-link">你要的免费Proxy资源全在这里了</a></h2><div class="post-info">May 27, 2019</div><div class="post-content"><p>地址：<a href="https://github.com/derekhe/ProxyPool" target="_blank" rel="noopener">https://github.com/derekhe/ProxyPool</a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在<a href="https://item.jd.com/12575102.html" target="_blank" rel="noopener">《爬虫实战：从数据到产品》</a>一书中，我讲到了一个基于ProxyBroker的代理池。经过我的长时间的实践，这个代理池用起来非常的方便和稳定。</p>
<p>基于<a href="https://github.com/constverum/ProxyBroker" target="_blank" rel="noopener">ProxyBroker</a>，增加了中国区域的代理资源。并引入了docker-compose，能够快速的方便的开始代理的抓取。</p>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a><a href="https://github.com/derekhe/ProxyPool#%E7%94%A8%E6%B3%95" target="_blank" rel="noopener"></a>用法</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose up</span><br></pre></td></tr></table></figure>

<p>然后浏览器打开<a href="http://localhost:8080/proxy.json" target="_blank" rel="noopener">http://localhost:8080/proxy.json</a> 即可得代理列表。每个代理都经过类型的验证，代理资源会随着时间增长。每个代理的有效期为一天时间。大概一天有1万左右的有效代理。</p>
<p>由于很多代理资源在中国无法访问的网站，部署在国内的服务器上会影响资源的获取，所以推荐将服务器部署到国外的服务器。服务器推荐使用<a href="https://m.do.co/c/4bc532e3ef94" target="_blank" rel="noopener">DigitalOcean</a>，我的多个服务器都在SFO2区域，非常的稳定。</p>
</div></article></div><div class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/post/2019-05-13-k3s/" class="post-title-link">使用k3s减少k8s成本</a></h2><div class="post-info">May 13, 2019</div><div class="post-content"><p>爱飞狗后台的数据爬虫以及数据服务器资源都部署在k8s上，使用rancher搭建。在不影响太多性能的情况下尽量选择最低配置的机器。对于内存不足的情况适当的使用交换文件代替（swap）。整个集群大致结构如下：</p>
<table>
<thead>
<tr>
<th>用途</th>
<th>机器类型</th>
<th>价格</th>
</tr>
</thead>
<tbody><tr>
<td>爬虫1</td>
<td>1G 1CPU</td>
<td>5美元</td>
</tr>
<tr>
<td>爬虫2</td>
<td>1G 1CPU</td>
<td>5美元</td>
</tr>
<tr>
<td>爬虫3</td>
<td>1G 1CPU</td>
<td>5美元</td>
</tr>
<tr>
<td>监控</td>
<td>1G 1CPU</td>
<td>5美元</td>
</tr>
<tr>
<td>etcd</td>
<td>2G 1CPU</td>
<td>10美元</td>
</tr>
<tr>
<td>rancher主机</td>
<td>2G 1CPU</td>
<td>10美元</td>
</tr>
<tr>
<td>aiflygo服务器</td>
<td>4G 2CPU</td>
<td>20美元</td>
</tr>
</tbody></table>
<p>一个月的开销大概在60美元左右。分析上面的集群可以发现，rancher和etcd两个节点是浪费了钱，每个月有20美元的开销。DigitalOcean提供了k8s的托管集群，可以将这部分开销节省。但托管集群的droplet无法定制化，无法使用交换分区和bbr，造成性能瓶颈。另外托管的droplet的最低要求也是2G的内存，造成不必要的开支。</p>
<p>k8s有一个非常不好的地方就是最低的机器要求比较高，1G内存的worker node已经完全低于推荐配置，如果在上面部署worker node直接的内存占用就要300M左右，剩余的内存空间并不多，必须要使用交换分区。etcd节点之前也用过1G的内存，但经常会由于大量使用交换分区导致性能问题，最后集群崩溃，所以无论如何也需要使用2G的内存才行。</p>
<p>最近rancher公司推出了k3s，其主打就是简易的部署和极地的机器消耗。这点对于节约成本来讲非常的重要。我试了下k3s的server大概只占用200M左右的内存，agent只占用几十兆内存，非常的节约。k3s也可以完全使用kubectl来进行管理，配置文件和k8s保持一致，非常方便。</p>
<p>我将etcd节点和rancher主机删除，替换成了1G 1CPU的机器(5美元)，节约了15美元，然后将aiflygo的服务器进行降配置到2G 2CPU (15美元)，总共节约20美元。得益于更多的可用内存，目前爬虫的性能比以前更好，整体集群的性能也非常的高。</p>
<p>至于HA，既然都穷到了用k3s来减少开销，对于我这样的小型的集群和不是关键系统来讲都不是需要考虑的了。</p>
</div></article></div><div class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/post/2019-04-17-book/" class="post-title-link">心路历程：爬虫实战——从数据到产品</a></h2><div class="post-info">Apr 17, 2019</div><div class="post-content"><p><img src="/images/2019/04-17/4372317-a5ecf0cdb82bfb23.png" alt></p>
<p>京东链接：<a href="https://item.jd.com/12575102.html" target="_blank" rel="noopener">https://item.jd.com/12575102.html</a></p>
<p>经过近一年的辛苦创作、编辑、等待，本书终于出版了。这种感觉有点像是十月怀胎，但没有生育时候的痛苦，只有最后得到的欣喜。现在回忆起去年接到写书的邀请，然后到纠结，再到刚开始痛苦的写作，以及最后成稿后的释然，一切都觉得是一场人生的经历。我倒是认为写书的目的不是为了赚钱，写一本书给自己，总结自己的过往，将经验传播给他人，就可以了。</p>
<h1 id="故事——还得从2017年说起"><a href="#故事——还得从2017年说起" class="headerlink" title="故事——还得从2017年说起"></a>故事——还得从2017年说起</h1><p>2017年1月左右，摩拜单车终于进入到成都。不像重庆那样的上山下坡，成都平原地势较平，成都原本就有非常多骑自行车的人，街头巷尾都有自行车的踪影。作为短途的必要交通工具，摩拜单车的进入算是给出行带来了非常大的方便。</p>
<p>有一天在查看摩拜单车的APP的时候，突发奇想是否可以将这些车的位置数据拿到，然后尝试分析一下运营状况，看看成都到底有多少车。打开电脑，轻车熟路的进行API分析，搞明白了API的接口，然后就写了一个简单的爬虫，获取了一个月左右的数据并进行了分析。然后在简书上发了几篇文章，并将源代码放在了<a href="https://github.com/derekhe/mobike-crawler" target="_blank" rel="noopener">https://github.com/derekhe/mobike-crawler</a>这个repo中。</p>
<p><img src="/images/2019/04-17/4372317-c00a3a025050c133.png" alt="image.png"></p>
<p>出乎意料的是，这些分析居然得到了非常大的流量，两三天时间就到了1万多的阅读量，甚至这个数据分析还“成功”的引起了对方的注意：被问到你怎么得到数据？</p>
<h1 id="2018年"><a href="#2018年" class="headerlink" title="2018年"></a>2018年</h1><p>2018年，顺着这个思路，后来我又继续分析并爬取了共享汽车、自由职业者网站，并继续坚持将2016年6月开始做的机票的数据爬虫做得更快更稳定。</p>
<p><img src="/images/2019/04-17/4372317-3cc428bc1c37c6b6.png" alt="image.png"></p>
<p>2017年末，爱飞狗旅行小程序上线，将我收集到的机票的数据公开，并集成了一个预测系统。知晓小程序对此特意写了一篇文章进行报道。随后，在2018年下半年ThougtWorks对外的YottaBytes分享中，我将爱飞狗的整个产品的规划、开发以及背后的技术实践都分享出来，并写成文章。</p>
<h1 id="写书？"><a href="#写书？" class="headerlink" title="写书？"></a>写书？</h1><p>2018年3月底，电子工业出版社的安娜编辑联系我，看我能不能约一本书稿。当时我还没放在心上，想想写个书算是一个大工程吧，费时费力的，当时就想想要不还是算了？后来和编辑的更多沟通中发现，爬虫方面的书最近还比较热，也能热一段时间，我做的这些工作也恰好和爬虫非常有关。</p>
<p>我随后看了一些目前的爬虫相关的书籍， 发现很多爬虫的书籍都写得很初级，讲讲Python的语法、讲讲几个库的用法，弄两个例子就完了，甚至有些书居然用了76页讲各种东西的安装！！初级的爬虫往往很简单，爬几个网站即可，但更复杂的如何去拿到app的数据，如何破解一些sign的思路，却全然没有。或许是太复杂了吧。即便有些数据拿到手了，怎么分析，怎么可视化，也很少有讲解。</p>
<p><img src="/images/2019/04-17/4372317-8bf24d9e42860a6e.png" alt></p>
<p>如果我要写的话，我一定不会写这样的书，我不会写初级的书。我要写的话，一定是从一个想法开始，到如何实现这个想法，到如何解决各种困难。案例的话，一定是end to end的，将数据达到实用的阶段。</p>
<p>还好之前做过的各种数据分析案例，都是有一定的业务背景的，有一些数据提供给了一些爱好者进行了更深入的分析，有一些数据甚至帮助一些公司进行一些新业务的拓展尝试。我和编辑沟通了这些想法，那就开始动手吧！（其实内心是有点纠结的，因为想到要写很长的时间，非常的难受）</p>
<p><img src="/images/2019/04-17/4372317-d5ef021e64217c0c.png" alt></p>
<h1 id="那就开始吧"><a href="#那就开始吧" class="headerlink" title="那就开始吧"></a>那就开始吧</h1><p>这是最原始的选题单，可以看到其实当时想写的例子其实比现在书中呈现的更多。但后来由于目标网站改动太大，以及有些网站的例子不太合适，所以进行了删减。</p>
<p><img src="/images/2019/04-17/4372317-3dddb7f3e254949c.png" alt></p>
<p>最后，只有半年的时间啊……</p>
<p><img src="/images/2019/04-17/4372317-a2c6b2d22ddae00e.png" alt></p>
<p>万事开头难，先写个提纲吧：</p>
<p><img src="/images/2019/04-17/4372317-8473927c57cb4b9a.png" alt></p>
<h1 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h1><p>工欲善其事必先利其器，2009年写过研究生的论文，倒是对Word玩的很溜。但如今都2018年了哟，好歹用点Markdown对吧！</p>
<p>好吧，就用Markdown。但是用Markdown的话，我需要把每个章节分开么？后来尝试了一下发现，单一Markdown文件就非常利于管理，不用有洁癖一样把东西拆来拆去：</p>
<ul>
<li>方便预览整书情况</li>
<li>方便章节之间相互引用</li>
<li>避免调整章节时候的麻烦事 </li>
</ul>
<p>然后，在Visual Studio中安装了Markdown Preview Enhanced插件，这个工具很好：</p>
<ul>
<li>自动预览</li>
<li>自动生成目录</li>
<li>自动内嵌图片、内嵌代码</li>
<li>能够导出成多种格式（Word格式、PDF格式等）</li>
</ul>
<p><img src="/images/2019/04-17/4372317-43052d93a0fabc33.png" alt></p>
<p>最后，整个项目当然会用git进行管理，这是最基本的啦。</p>
<p>其中我觉得需要拿出来讲的是，Markdown中可以直接引用代码。这意味着我一边写书一边写的代码，可以无缝的集成到文章中，这样我更改了代码以后，书中的代码也更新了，避免了不同步的问题。</p>
<p><img src="/images/2019/04-17/4372317-ae07852f73432856.png" alt></p>
<h1 id="一年"><a href="#一年" class="headerlink" title="一年"></a>一年</h1><p>现在回顾一下整个出书的历程。</p>
<p>我这个人是喜欢一鼓作气做完一件事情。为了保证及时交付，每天都分配了至少两个小时的时间，再加上之前已经有一些素材的积累，所以整个书的书写都相对比较快。基本上在4个月时间内集中写完了书籍。初稿完成后，我的事情就比较少了，主要是一些校对的工作。</p>
<ul>
<li>4月底：建立Repo</li>
<li>5月初：书写第一章</li>
<li>6月初：完成单车部分书写</li>
<li>6月底：完成共享汽车部分书写</li>
<li>7月底：完成Freelancer部分书写</li>
<li>8月初：增加爱飞狗产品</li>
<li>9月底：完成所有书写，并转换成docx供编辑修改</li>
<li>10月初——12月底 一排</li>
<li>12月底到1月初： 二排，拿到书号</li>
<li>2月底：终审</li>
<li>3月中：封面、查重、定价</li>
<li>3月底：样书到</li>
</ul>
<h1 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h1><ul>
<li>持续和编辑联系，写完一章就给编辑大致看看得到反馈<br>这一点避免了很多走弯路。第一次写书，对写书需要做的事情了解的比较少，加之平时在敏捷项目中都强调快速反馈，在写书过程中也是，能够及时避免一些坑。</li>
<li>一鼓作气，坚持每天花些时间写，避免拖拉</li>
<li>长期写博客，积累素材</li>
</ul>
<h1 id="坑"><a href="#坑" class="headerlink" title="坑"></a>坑</h1><p>在写书的过程中也有一些不可避免的“坑”。一来是写书不像是写博客那么随意，对于语言以及内容的提供都要有一个监管在里面。运气不好的是初稿交了以后，编辑告诉我2018年国家对地图相关的图片管理很严。我了解到相关的书籍都必须要进行审核，虽然说是免费，但是为了避免审核的时间消耗和一些潜在的风险，我们对书中的一些地图相关的图片、内容进行了一些修改和删减。</p>
<p>爬虫这种技术有一定的法律风险，再三考虑之下对书中也有提到一些特定APP名字的地方，我们将名字进行了打码，并强调了这些思路以及代码都仅供参考。</p>
<p>初稿的反馈说书写的很直白，全是干货，审核后说是要语言柔和一些。由于这个书的架构和其他类似书籍先将一大堆基础知识不一样，出版社的领导提议说将书的结构进行调整，变成介绍工具在前，案例在后。但这就是我想尽力避免的，不想让读者的钱花到了原本网上可以很快查到的地方，所以拒绝了。</p>
<p>由于互联网时代变化很快，网站和APP都在改版，所以爬虫相关的代码，目前有些已经无法使用了。这是预料之中的事情，但书中所传递的方法，是通用的。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>不管如何，第一本书也算是出来了，放在书架上也是对我的极大地鼓舞。写书不容易，一旦开始就要一鼓作气。平时的积累非常重要，所以多多写博客吧。</p>
</div></article></div><div class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/post/2018-07-26-aiflygo/" class="post-title-link">从数据到产品——爱飞狗背后的故事</a></h2><div class="post-info">Jul 25, 2018</div><div class="post-content"><p>这篇文章基于最近整理的一份演讲的Slide，由于报名太晚错过了截止日期，所以只好写成文章，一起来看看爱飞狗背后的一些故事。</p>
<p><img src="/images/2018/07-26/4372317-845e9f8c73863592.jpeg" alt></p>
<p>几年前我和家人会经常往返于成都和广州两个城市，从平时的观察中可以看到机票价格从400人民币到全价接近1500人民币，机票产生的波动有时候会高达100元以上，如果没有看好时间，一家人出行就会增加几百元的成本。</p>
<p><img src="/images/2018/07-26/4372317-09d583c39cba3b63.jpeg" alt></p>
<p>买机票的时候，我应该和大家差不多，会在买机票前进行一番观察。主要是观察未来几天起飞的机票价格，用来选择哪天出行。然后手工记录一下近期的价格变化，看看是不是在涨价还是跌价中。当然这个纯属体力活也容易错过最佳购票时期。我当时在想，如果我能知道去年的机票的价格和变化趋势，或许可以用来预测今年的价格的波动和判断是否需要购买机票。</p>
<p><img src="/images/2018/07-26/4372317-6c33989450778817.jpeg" alt></p>
<p>我提出了两个问题，近期的价格的趋势是什么？决定好起飞时间后，我应该提前多少天购票？</p>
<p><img src="/images/2018/07-26/4372317-c8d8f8d74d0551bf.jpeg" alt></p>
<p>我查了一下目前我们有的一些服务和APP，看是不是有人已经做过了类似的事情。</p>
<ul>
<li>Farecast是最早做机票预测服务的一家公司，后来被微软收购后并入了Bing，此后Google收购了Farecast所依赖的数据源后，Bing的这项业务也停止了，所以现在Farecast已经关闭，无法使用。</li>
<li>Kaya这家网站提供了机票的近期价格曲线以及购票建议，十分切合我的需要，但遗憾的是中国的数据源非常少，几乎没有可参考性。</li>
<li>Hopper也是最近两三年比较好的一款App，提供了出行的建议以及是否购票的提示。但Hopper不会提供去年的曲线图，也没有近期的票价情况，一切看你是否相信它的AI预测了。和Kaya类似，这个App也没有中国区域足够的数据，没有什么参考价值。</li>
</ul>
<p><img src="/images/2018/07-26/4372317-0ebfd0403e9f3169.jpeg" alt></p>
<p>面对这样的情况，作为一个程序员，我觉得是时候出手了。</p>
<p><img src="/images/2018/07-26/4372317-3883b9a01aa5868f.jpeg" alt></p>
<p>由于我没有运营机票代理业务，所以无法获得机票信息的一手资源。对于我来讲唯一能够获取到数据的方式就是采用爬虫，通过模拟人工的搜索来获得票价信息，存储到数据库中。</p>
<p><img src="/images/2018/07-26/4372317-bb86124bb4e2b527.jpeg" alt></p>
<p>为了分析一整年的数据，我在给自己定了一些目标：</p>
<ul>
<li>至少要连续采集1年的数据</li>
<li>更新频率大概在1小时左右</li>
<li>低成本</li>
<li>容忍爬虫或者数据源的失败</li>
</ul>
<p><img src="/images/2018/07-26/4372317-6efa8a1e343fbca9.jpeg" alt></p>
<p>根据这些目标结合当时的情况，我将这个任务分解成了以下几个部分：</p>
<ul>
<li>机票采集的范围：中国范围内2800个航线的直飞的价格数据</li>
<li>采集的数据：每条航线起飞前45天的票价信息</li>
<li>数据源：4个数据源做互补，一来是为了检查数据的准确性，二来是为了防止数据源失效</li>
<li>全年无休连续爬取</li>
</ul>
<p><img src="/images/2018/07-26/4372317-cdc01bb1f2bc4720.jpeg" alt></p>
<p>在数据爬虫的架构上，我需要一个低成本的方案，毕竟要干上一年以上的时间（其实到现在已经2年多了）。我并没有使用各种爬虫的框架，而是自己写了一个轻量级的爬虫来处理代理的筛选逻辑和爬取数据的验证。这些爬虫将会运行在DigitalOcean或者Linode的$5的服务器上，这些服务器上只有1个CPU和1G的内存。自定义的爬虫能够尽可能的根据性能进行针对性的调优，从而增加性能。每个爬虫之间互相独立，部署在单独的机器上，也是为了减少爬虫之间的相互干扰。</p>
<p>在存储上我是用了tar.xz的文件将所有的爬取到的原始文件按照每一轮爬取进行打包压缩，然后在本地里面的一台PC机负责将所有的数据拉回来，放到一个4T的硬盘上。同时这些数据也会存储到百度云上面进行备份。目前这些压缩包已经达到2T左右了，解压下来大约有20T的数据量，大约有380亿条数据。</p>
<p>在数据处理上，首先是实时数据流的处理。使用了kafka来进行队列管理，一个Parser来进行实时的处理原始数据，将处理好的结果放到redis缓存中，供小程序的后端使用。</p>
<p>在数据的分析上，我使用Java手写了一个高性能的ETL，直接读取压缩包然后进行数据解析。在这里也是使用了轻量的原则，没有使用更多框架，从而达到非常高的性能。</p>
<p>在PC机上也使用了Postgres中进行数据存储，每一天的数据存储到一个表中。</p>
<p><img src="/images/2018/07-26/4372317-38e021ce2918f5ab.jpeg" alt></p>
<p>在近两年的时间中我积累了以下一些经验：</p>
<ul>
<li>在长时间的爬取中证明了简单的爬虫的可变性非常好，在针对特殊场景调优的时候非常有针对性。</li>
<li>存储原始文本也为了应对数据结构的变化，当数据结构变化后，本地的Parser失败以后，可以重复的调用之前的数据，从而保证数据不丢失。</li>
<li>多个数据源进行备份的设计非常好。目前依然有两个数据源在持续爬取。针对两个数据源我也可以进行数据的对比，发现数据对比后数据较为相似，说明没有爬取到脏数据。</li>
</ul>
<p><img src="/images/2018/07-26/4372317-c3723d433a95f688.jpeg" alt></p>
<p>拿到这些数据后，就要开始数据分析了。</p>
<p><img src="/images/2018/07-26/4372317-919ca4f2a5132147.jpeg" alt></p>
<p>分析的结论在我的简书的前几篇文章中都有提到。这里不再赘述，请参看：<br><a href="https://www.jianshu.com/p/3b22fb101189" target="_blank" rel="noopener">https://www.jianshu.com/p/3b22fb101189</a><br><a href="https://www.jianshu.com/p/366e839af3ea" target="_blank" rel="noopener">https://www.jianshu.com/p/366e839af3ea</a></p>
<p><img src="/images/2018/07-26/4372317-fbf60a7e84dfd7a8.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-6effc58faf6e16e0.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-e5cb6b59626330f3.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-66b3641b3d4c1e27.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-79ab3aa65bf96b24.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-19e11a47f31fd3e0.jpeg" alt></p>
<p><img src="/images/2018/07-26/4372317-935fbad9889c8acd.jpeg" alt></p>
<p>在分析完这些数据后，我们再回头看一下最初问题的定义：针对某一条航线，给定历史价格和近期的价格，现在看到当前的价格是P和距离起飞还有N天，我们应该是买还是等待？</p>
<p><img src="/images/2018/07-26/4372317-e6b798b79cd6fb73.jpeg" alt></p>
<p>我使用了以下一些算法来分析，包括决策树、支持向量机、Q-learning和神经网络。</p>
<p><img src="/images/2018/07-26/4372317-95fc880ae8eafb40.jpeg" alt></p>
<p>在准备数据时，提取了以下一些基本特征，当然针对不同的算法还是用不同的特征。</p>
<p><img src="/images/2018/07-26/4372317-96d4609713a24521.jpeg" alt></p>
<p>在数据的分割上，考虑到这个是时间序列相关的数据，将数据集分为1年的训练数据，2个月的测试数据和2个月的验证数据集。当然实际数据比这个多，可以尝试更多的组合。</p>
<p><img src="/images/2018/07-26/4372317-28601a2ea2179098.jpeg" alt></p>
<p>但这些算法算出来的结果还并不令人特别满意，达到的准确率也只能超过人工根据一些简单的规则推导出来的准确率，可能和我所用的数据训练方式有关。所以在线上计算的时候，我目前还是采用专家系统的做法，将数据分析的结果整理成规则，根据这些规则还是能够得到较高的预测准确率，其他的指标例如Recall等可能会比较差。</p>
<p><img src="/images/2018/07-26/4372317-71607194c985ffa1.jpeg" alt></p>
<p>那么为什么专家系统能够行得通呢？我从一些内部人士了解到，原因在于这些机票都是来源于中航信，而这个系统比较老旧，针对价格的调整也不够灵活，大多数都是给予一些人工的规则来进行。收益管理员基本上都是从这里学到一些规则然后进行调价，所以学到的规则其实比较相近，所以会产生大家都有差不多一样的规则。</p>
<p><img src="/images/2018/07-26/4372317-a369324117452e1d.jpeg" alt></p>
<p>有了数据之后，想到可以来造福大众，便产生了做一个小产品的想法。</p>
<p><img src="/images/2018/07-26/4372317-7dfb7d987bc7eb50.jpeg" alt></p>
<p>这时候正直小程序比较火的年代，小程序比较简单，而且受益于微信的用户群可以很方便的进行推广。所以设计了几个简单的功能，能够选择出发、到达和起飞日期，然后会显示出实时的价格和历史最低的价格，点进去后可以看到预测、近期的价格波动和去年的价格波动。这能够更好的指导大家进行机票的选择。</p>
<p><img src="/images/2018/07-26/4372317-c081eee454b67914.jpeg" alt></p>
<p>下面来看看产品背后的架构：</p>
<ul>
<li>前端当然是用微信提供的一套进行开发，这个和其他的小程序没有什么两样。</li>
<li>后端使用Flask进行开发，简单快捷。</li>
<li>数据提供方面有之前提到的实时数据的显示，来自Redis缓存，当然还有离线历史数据数据。这些数据每天从离线的PC机上同步到云端，然后由API进行展示。</li>
<li>还有用户行为数据的存储及分析。这些数据都存在ElasticSearch服务器中以便进行快速的搜索，借助Kibana可以对用户行为进行在线的分析，非常的方便。当然还有一些和用户相关的需要实时更新的数据（例如session值和飞币的信息）也放在EleasticSearch中，主要是利用了NoSQL数据库没有Schema的特点，增加数据使用的灵活性。</li>
<li>基础设施方面小程序相关的部分全部在阿里云的服务器上。所有的服务都使用Kubernetes进行管理，Rancher进行可视化管理，爬虫等服务放在DigitalOcean一遍节约成本。</li>
</ul>
<p><img src="/images/2018/07-26/4372317-67f8a98ca4a846e2.jpeg" alt></p>
<p>小程序也发布了9个多月了，目前积累了大约4.3万的用户，数量还相对较少，这个小程序的未来如何考虑呢？</p>
<p><img src="/images/2018/07-26/4372317-063e5114afe70da9.jpeg" alt></p>
<p>首先是继续将机器学习应用到价格的预测，毕竟积累了2年的多的数据是一笔不小的财富。<br>然后是提供更好用的界面提升用户的使用满意度。<br>最后就是找到一套可行的商业模式进行变现。目前小程序还只能依靠广告有一点收益，完全是入不敷出的情况。</p>
<p><img src="/images/2018/07-26/4372317-c219d1d44d455f0d.jpeg" alt></p>
<p>如果您对这个小程序感兴趣，请试试微信搜索“爱飞狗旅行”和关注微信公众号“爱飞狗”，或者扫一扫下面的二维码。您的支持将是我进步的最大的动力。</p>
<p><img src="/images/2018/07-26/4372317-f1ff858612c3b36d.jpg" alt></p>
</div></article></div><div class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/post/2018-06-22-look-into-your-business/" class="post-title-link">小心！我正在偷窥你的运营</a></h2><div class="post-info">Jun 22, 2018</div><div class="post-content"><p>首先，问一个很简单的问题考考你——在上海，摩拜单车出行的高峰时段是什么？这个问题相对比较容易回答，根据普通人上下班的时段应该是早上七八点左右，下午六七点左右。恭喜，你答对了。</p>
<p>现在问一个稍微更难一点的问题：在上海，摩拜单车使用次数的分布是什么情况？对于我而言，基本上每天就从地铁站骑到公司，然后下班从地铁站骑回公司，就算做是两次吧，通常这部分人应该不少。而这道题问的是分布，那么得知道0次骑行的车的数量，骑行一次、两次、n次的车的数量。我们可以做个小范围的采样，在单车密集的区域架设几个摄像机，然后分析一下哪些车没有被骑走即可。但是，骑行两次、三次以及更多次数的车的数量如何得知呢？我们很难知道，或许我们可以通过发放调查问卷来得知，但如果样本太少会得到打的偏差。同样的道理，回答“摩拜单车的骑行距离的分布式什么情况”，也相对较难。</p>
<p>要知道共享单车的运营情况，我们通常只能通过官方或者研究机构的各种报告来获取到。然而这些报告可能缺少一些时效性，对某些研究来讲并不能产生任何作用：例如在城市规划时研究OD线、叠加图层分析更多因素的结合时，就显得无能为力。最后的渠道也是最正规的渠道是通过和共享单车企业合作，然而由于涉及到商业机密，通常很难获得。</p>
<blockquote>
<p>OD线（Origin-Destination Line），是指连接“起点”（origin）和“终点”（destination）的线，通常用于在地图上直观表示某两个地理位置之间的联系，如车辆的行驶轨迹、飞机的航线和人口的迁徙等。</p>
</blockquote>
<p>回到单车的APP上来，既然能够帮我们显示出车辆的信息，那么我们能不能把一个区域的范围的车的信息都抓取下来，然后进行分析呢？这个思路非常的有趣，在我之前写的<a href="https://www.jianshu.com/p/07225f301fc4" target="_blank" rel="noopener">摩拜单车爬虫解析——找到API
</a>中已经有所涉及。在这里我们不谈技术细节，通过对爬取的上海市2018年5月29日星期二（晴天）的数据进行分析我们迅速地得出上面问题的答案：</p>
<ul>
<li>单车的分布图和部分热力图。可以明显的看到上海城市的发展规模以及人口密集区域的分布。</li>
</ul>
<p><img src="/images/2018/06-22/4372317-94b42d986f876bb9.png" alt="单车分布图"></p>
<p><img src="/images/2018/06-22/4372317-b6e3813ae240f26c.png" alt="单车热力图"></p>
<ul>
<li><p>单车使用次数类似一个正则分布，2次的次数最多，符合人们的直觉。<br><img src="/images/2018/06-22/4372317-b23368fcf3e5a0f0.png" alt="使用次数直方图"></p>
</li>
<li><p>骑行的直线距离分布在2千米之类，以1千米范围为最多，这通常也是单车解决“最后一公里”问题的证据。<br><img src="/images/2018/06-22/4372317-c7aca8e9578a7763.png" alt="骑行距离分布"></p>
</li>
<li><p>出行的时段的分布在早上7点左右达到高峰，然后晚上7点左右达到高峰，这和人们的出行习惯是密切相符的。有趣的是夜间22点以后到第二天凌晨5点左右，也有单车在移动，但这时候骑行的人相对很少，所以这些单车的移动很有可能是摩拜单车的运营人员在批量的挪单车。如果将这些数据加以分析应该可以知道这些车从哪里挪到哪里，进而更深层的分析单车企业的运营策略。</p>
</li>
</ul>
<p><img src="/images/2018/06-22/4372317-0e7ba730ae95761c.png" alt="骑行时段分布"></p>
<p>同样的道理，让我们来看看共享汽车的运营情况。我们抓取了去年GoFun从10月到1月的数据，得出了以下一些运营状况的分析：</p>
<ul>
<li>GoFun绝大多数车型都是奇瑞的车。两座和四座的车是主流，车身小巧一方面比较节能，另一方面比较容易操作。这主要是因为奇瑞投资了GoFun。</li>
</ul>
<p><img src="/images/2018/06-22/4372317-306cf2bd22323813.png" alt="车辆信息"></p>
<ul>
<li>截止2018年1月26日，GoFun目前在全国总共有10327辆车。下图是车辆增长的情况。可见运营一直在持续，并且缓慢增长。</li>
</ul>
<p><img src="/images/2018/06-22/4372317-a4106cb1e08baee0.png" alt="增长"></p>
<ul>
<li>三个月内车的使用次数的分布。横坐标是使用的次数，纵坐标是次数对应的车的数量。近似一个正态分布，大约70%的车都在24到72的区间。平均起来每天0.3次到1次左右，这对于共享汽车企业来讲并不是一个好事，太低的使用率可能会导致入不敷出。</li>
</ul>
<p><img src="/images/2018/06-22/4372317-1b9fddc4f1699006.png" alt="image"></p>
<ul>
<li>使用时长的分布。横坐标代表停车的时长（小时）最长统计到70小时，纵坐标代表有多少车次。对比停车时长的分布，使用时长的分布比较明显的集中在0到6小时之间，这也是共享出行的特点。由于GoFun有包天的租车服务，所以长期的出行的费用也是可以接受，长达70小时以上的使用时间也有1700多车次。</li>
</ul>
<p><img src="/images/2018/06-22/4372317-86fd7bfd12e67d08.png" alt="image"></p>
<p>由于篇幅所限，更多的分析结果可以参见<a href="http://www.april1985.com/sharecar_report/">大数据看共享汽车</a>一文。</p>
<h1 id="保护我们的数据"><a href="#保护我们的数据" class="headerlink" title="保护我们的数据"></a>保护我们的数据</h1><p>以共享单车为例，当我们手工的小范围的采集数据时，并不能产生很大的作用，当我们收集到的数据越来越多时，甚至通过自动化爬虫这种手段获取到足够多的数据以后，大数据就开始展现量变到质变的过程。共享单车、共享汽车的案例中，虽然我们没有得到订单的数据、没有用户信息，但通过对海量数据分析、结合多种维度，我们依然可以窥探企业的运营。我在<a href="https://www.jianshu.com/p/35d6ff3f0b7b" target="_blank" rel="noopener">2017自由职业大数据分析</a>一文中爬取到Freelancer网站的所有公开信息并进行了自由职业的分析；在<a href="https://www.jianshu.com/p/3b22fb101189" target="_blank" rel="noopener">机票大数据分析，揭示购票的秘密
</a>中，通过一年多的机票价格数据采集得到多个机票购票的建议。所有的数据都来自于公开的API，而主要的获取方式则是爬虫。</p>
<blockquote>
<p>网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。</p>
</blockquote>
<p>爬虫获取信息的方式可以通过爬取页面，也可以通过分析APP的请求来得到API。所以一切返回数据的地方都是可以通过一些手段获取到，而主要的区别在于获取数据的难度的大小，这也通常是爬虫和反爬虫的较量之一。当爬虫的成本高到一定水平以后，反爬虫就胜利了。</p>
<p>通常来讲，反爬虫的手段无外乎对IP限制、验证码、要求登录等，可以参考<a href="https://segmentfault.com/a/1190000005840672" target="_blank" rel="noopener">关于反爬虫，看这一篇就够了</a>一文。固然这些方式都可以较好的保护应用，但会给适用方带来不便，除此之前我们可以考虑一些建议来减小数据被大规模爬取的可能，增加爬虫的成本：</p>
<h2 id="从业务上进行考虑，不需要客户感知的数据不提供或者少提供"><a href="#从业务上进行考虑，不需要客户感知的数据不提供或者少提供" class="headerlink" title="从业务上进行考虑，不需要客户感知的数据不提供或者少提供"></a>从业务上进行考虑，不需要客户感知的数据不提供或者少提供</h2><p>以单车应用来看，ofo单车并没有提供摩拜单车的预定的功能，所以在界面上也没有必要展示出单车的编号信息。在去年9月份左右，ofo单车对API进行了调整，将所有的单车编号进行随机化处理，导致ofo单车的数据目前无法爬取。哈罗单车在未登陆的情况下只提供几台单车的数据，登陆后能够展示附近所有的单车。由于需要登录，爬虫的成本会变得很高也非常的脆弱。<br>反观某大型旅游OTA网站的API，调用后返回了相当数量的额外信息，甚至在价格方面包含了两种价格，而其中一种价格并不会在界面上进行显示，这种信息非常值得怀疑是不是内部的价格。</p>
<h2 id="盘点所有的API的使用方，谨慎一切以JavsScript开发的客户端"><a href="#盘点所有的API的使用方，谨慎一切以JavsScript开发的客户端" class="headerlink" title="盘点所有的API的使用方，谨慎一切以JavsScript开发的客户端"></a>盘点所有的API的使用方，谨慎一切以JavsScript开发的客户端</h2><p>目前手机APP层面的保护措施已经相对较多，而且大多数应用受到加密外壳的保护很难破解，想得到URL里面的签名信息的生成方式变得非常困难。然而仔细盘点一款产品的各种渠道，会发现有些产品可能会提供HTML5、公众号、小程序等入口，而这些以JavsScript生态圈的应用并未收到很好的保护。<br>以某款快递软件为例，该查询软件中提供了sign信息，基本无法知道如何生成，但在小程序中调用了类似的API并带上了sign信息，通过一个Root的安卓手机可以将小程序解包，分析JavaScript很容易的发现了sign的生成方式。另外某单车软件登陆的签名信息eption也是经过了加密处理，在小程序中也能够发现其生成的方式，马奇诺防线就这样轻松的被越过了。<br>当然小程序端的防止爬取的方式比较简单，小程序的后端可以通过微信的接口生成动态的token的方式进行用户验证，防止数据泄露。</p>
<h2 id="做好后台的监控"><a href="#做好后台的监控" class="headerlink" title="做好后台的监控"></a>做好后台的监控</h2><p>道高一尺魔高一丈，后台API调用的日志和监控手段要重视。对于异常数量的API调用要及时分析出原因减少被利用的可能。</p>
</div></article></div></div></div><div id="sidebar" class="left"><div id="about-panel"><div class="info"><div class="title">贺思聪</div><div class="bio">《爬虫实战：从数据到产品》作者，国内首个机票价格历史及预测小程序“爱飞狗旅行”作者，极客、架构师、大数据玩家。</div></div><img src="/images/avatar.png" class="avatar"></div><div id="friend-links-panel"><div class="panel-title"><p>LINKS</p></div><a href="http://github.com/derekhe" target="_blank" class="link-item"><div class="link-container"><div class="site-name">GitHub</div><div class="site-desc">http://github.com/derekhe</div></div></a><a href="http://www.jianshu.com/u/b1e713e56ea6" target="_blank" class="link-item"><div class="link-container"><div class="site-name">简书</div><div class="site-desc">http://www.jianshu.com/u/b1e713e56ea6</div></div></a><a href="https://www.linkedin.com/in/sicong-he-56a95437/" target="_blank" class="link-item"><div class="link-container"><div class="site-name">LinkedIn</div><div class="site-desc">https://www.linkedin.com/in/sicong-he-56a95437/</div></div></a></div></div></div><div class="clear"></div></main><footer class="footer-container"><div class="paginator"><a href="/" class="prev">PREV</a><a href="/page/3/" class="next">NEXT</a></div><div class="copyright"><p>© 2008 - 2020 <a href="http://www.april1985.com">贺思聪</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/AngryPowman/hexo-theme-prontera" target="_blank">hexo-theme-prontera</a>.</p></div></footer><script>var _mtac = {}; (function() {     var mta = document.createElement("script"); mta.src = "http://pingjs.qq.com/h5/stats.js?v2.0.2";    mta.setAttribute("name", "MTAH5");    mta.setAttribute("sid", "500490740");    var s = document.getElementsByTagName("script")[0];    s.parentNode.insertBefore(mta, s);})();</script></body></html>